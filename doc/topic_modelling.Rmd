---
title: "topic_modelling"
author: "Hui Chiang"
date: "21/02/2019"
output: html_document
---

```{r}
#Topic Modelling
library(tm)
library(topicmodels)
jobs <- read.csv('/Users/tayhuichiang94/Downloads/job.csv', header=TRUE)

#Create corpus
docs <- Corpus(VectorSource(jobs$Preferred.Skills))

#Preprocessing
#Transform to lower case
docs <-tm_map(docs,content_transformer(tolower))

#Remove symbols
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " " , x))})
docs <- tm_map(docs, toSpace, '-')
docs <- tm_map(docs, toSpace, '’')
docs <- tm_map(docs, toSpace, '‘')
docs <- tm_map(docs, toSpace, '•')
docs <- tm_map(docs, toSpace, '”')
docs <- tm_map(docs, toSpace, '“')
docs <- tm_map(docs, toSpace, 'â')
docs <- tm_map(docs, toSpace, '€')
docs <- tm_map(docs, toSpace, '¢')
docs <- tm_map(docs, toSpace, '™')

#Remove punctuation
docs <- tm_map(docs, removePunctuation)

#Strip digits
docs <- tm_map(docs, removeNumbers)

#Remove stopwords
docs <- tm_map(docs, removeWords, stopwords('english'))

#Remove whitespace
docs <- tm_map(docs, stripWhitespace)

#Check document
#writeLines(as.character(docs[[30]]))

#Stem document
docs <- tm_map(docs,stemDocument)

#Define and remove more stopwords
myStopwords <- c('can', 'say','one','way','use','also','howev','tell','will','much','need',
                 'take','tend','even','like','particular','rather','said','get','well',
                 'make','ask','come','end','first','two','help','often','may','might','see',
                 'someth','thing','point','post','look','right','now','think','‘ve ','‘re',
                 'anoth','put','set','new','good','want','sure','kind','larg','yes,','day',
                 'etc','quit','sinc','attempt','lack','seen','awar','littl','ever','moreov',
                 'though','found','abl','enough','far','earli','away','achiev','draw','last',
                 'never','brief','bit','entir','brief','great','lot', 'skill', 'abil','work',
                 'must','strong','experi','candid','prefer','possess','requir',)

docs <- tm_map(docs, removeWords, myStopwords)

#Inspect a document as a check
#writeLines(as.character(docs[[30]]))

#Create document-term matrix
dtm <- DocumentTermMatrix(docs)

#Change rownames
rownames(dtm) <- jobs$Job.ID

#Collapse matrix by summing over columns
freq <- colSums(as.matrix(dtm))

#Length should be total number of terms
length(freq)

#Create sort order (descending)
ord <- order(freq,decreasing=TRUE)

#List all terms in decreasing order of freq and write to disk
freq[ord]

#Set Gibbs Sampler parameters
burnin <- 5000
iter <- 3000
thin <- 500
seed <-list(2003,5,63,100001,765)
nstart <- 5
best <- TRUE
k <- 10

#Run LDA using Gibbs sampling
raw.sum=apply(dtm,1,FUN=sum)
dtm=dtm[raw.sum!=0,]
ldaOut <-LDA(dtm,k, method='Gibbs', control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin))

#Docs to topics
ldaOut.topics <- as.matrix(topics(ldaOut))
write.csv(ldaOut.topics,file=paste(“LDAGibbs”,k,”DocsToTopics.csv”))

#Top 10 terms in each topic
ldaOut.terms <- as.matrix(terms(ldaOut,10))
write.csv(ldaOut.terms,file=paste(“LDAGibbs”,k,”TopicsToTerms.csv”))

#Probabilities associated with each topic assignment
topicProbabilities <- as.data.frame(ldaOut@gamma)
write.csv(topicProbabilities,file=paste(“LDAGibbs”,k,”TopicProbabilities.csv”))

#Find relative importance of top 2 topics
topic1ToTopic2 <- lapply(1:nrow(dtm),function(x)
sort(topicProbabilities[x,])[k]/sort(topicProbabilities[x,])[k-1])

#Find relative importance of second and third most important topics
topic2ToTopic3 <- lapply(1:nrow(dtm),function(x)
sort(topicProbabilities[x,])[k-1]/sort(topicProbabilities[x,])[k-2])
```